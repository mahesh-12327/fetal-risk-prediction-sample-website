<!DOCTYPE HTML>
<html>
<head>
	<title>Terminologies</title>
</head>
<body>
	<h4><a href="C:\Users\Mahesh\Desktop\html\htmlproject\Project Harsha.html">back</a></h4>
	<b>Terminologies</b><br>
	<p><b>MRMR:</b> Minimum Redundancy Maximum Relevance(MRMR) technique is used to select features that have high class correlation & low correspondence with the other features in dataset.With our given data set If we use mrmr feature selection algorithm then it take’s out the most important features out of all features and it’s used for the predicting the result with test dataset. While selecting the feature’s before it generates the correlation matrix to check the feature which are highly effective in dataset. Later on it gives the filtered features from all the features. With these from train set and test set we use these features to predict the result. Mutual Information Difference Criterion & Mutual Information Quotient Criterion represent the difference (or) the appropriate and redundant quotient, respectively, are two widely used target function types. The approach to mRMR selection of features for temporal data includes certain preprocessing techniques which flatten temporal data into a single matrix in 	advance. This can lead to the lack of potentially valuable information between time data (such as temporal order information).<br>
	<b>PCA:</b>Reduce data from m-dimensions to n-dimensions. Primary component analysis
	is a technique to reduce dimensionality of these datasets which are increasing their inter pretability while reducing  loss of information in the same amount of time. It did by generating new variables which are  uncorrelated that increase the variance successively and Seeking these new variables reduces key components to address an problem of own value / selfvector, and new variables are determined by dataset through hand which are not  priori and making PCA an efficient data analysis strategy. In another sense this is also usable, Since these variants of technique were developed, which are customized to different data types and structures. In PCA feature Selection we use two type of objectives one is standard scalar and the other is minmax scalar which is similar like feature scaling and the normalization which is used find the feature range by compressing into a limit it makes the algorithm to work faster .In this the data is done with the pca.fit and later it transforms the features.<br>
	<b>Random Forest:</b>Yet one thing to remember is that constructing the forest is not the same as setting up the decision using the information gain or index gain strategy. Random forests (or) random decision forests which are an system of ensemble learning  for classification , regression & the other activities that functions by constructing a mixture of decision trees during training time & outputting class which would be the class mode (classification) (or) regression of individual trees and the 
	Random decision-making forests are perfect for decision trees' ability to overfit to their training set.<br>
 	<b>Decision Tree:</b>A Decision tree is just a tree like flowchart whereby each internal node represents an attribute test, each branch depicts a test result, and that each leaf node (terminal node) is labelled with a label. Here in this data set the decision tree algorithm it works in tree structured manner checking from parent node whether it’s yes or no then it goes with it’s child nodes yes or no and continue through out the tree and gives the output.<br>
	<b>SVM:</b>The aim of (SVM)support vector machine algorithm is to find a hyper-plane in an N-dimensional space which separately classifies the distinct data points.
	SVM’s are supervised learning models in machine learning which are associated learning algorithms which evaluate data which are used for the classification & regression analysis. Based on a collection of training instances, each of which is classified as belonging to either one or both of two categories, a Svm classification algorithm creates model which assigns 
	the training examples to one or both category, rendering it a non - probability binary linear classification (though methods like Platt scaling occur to use Support vector machine in classification). The Support vector machine model is a description of the instances as spatial points, mapped in such a way as to split the examples of the different groups by as large a simple distance as possible.<br>
	<b>Navies Bayes:</b>Naive bayes classifiers are the family of basic "probabilistic classifiers" regarding the interpretation of bayes with clear (naive) assumptions of the characteristics of freedom. 
	Naive Bayes is a simple that construct & particularly useful for extremely big data sets and Naive- Bayes is considered to out perform also extremely advanced methods of classification.Naive Bayes Theorem gives a way for P(c), P(x) and P(x) to measure posterior likelihood. <br>
	<b>Multi Layer Perception:</b>This is composed of at least 3 node layers which are the input layer,output layerand a hidden layer. In the exception of input nodes , each node becomes a neuron using a nonlinear activation mechanism. For teaching, MLP uses a supervised method of learning called backpropagation.
	</p>
</body>
</html>